{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e551019-3753-4496-b15c-94b48825fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('logs')\n",
    "\n",
    "SEED=1\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "cudnn.deterministic = True\n",
    "\n",
    "GPU = 'cuda:0'\n",
    "START_EPOCH = 0\n",
    "ARCH = 'densenet'\n",
    "EPOCHS = 10\n",
    "LR = .0005\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "PRINT_FREQ = 50\n",
    "BATCH_SIZE = 100\n",
    "WORKERS=2\n",
    "#TRAINDIR=\"data/training/DEWP_class\"\n",
    "#VALDIR=\"data/test/DEWP_class\"\n",
    "imagenet_mean_RGB = [0.47889522, 0.47227842, 0.43047404]\n",
    "imagenet_std_RGB = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b14cd7a-c35d-46c8-80f9-10fdf2b15138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        \n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, scaler, epoch):\n",
    "\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top2 = AverageMeter('Acc@2', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top2],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "                \n",
    "        images = images.cuda(non_blocking=True)\n",
    "        target = target.cuda(non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        acc1, acc2 = accuracy(output, target, topk=(1, 2))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top2.update(acc2[0], images.size(0))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if i % PRINT_FREQ == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "            writer.add_scalar('Train Acc@1',\n",
    "                acc1[0],\n",
    "                epoch * len(train_loader) + i)\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top2 = AverageMeter('Acc@2', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top2],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            \n",
    "            images = images.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            acc1, acc2 = accuracy(output, target, topk=(1, 2))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top2.update(acc2[0], images.size(0))\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % PRINT_FREQ == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "                writer.add_scalar('Validation Acc@1',\n",
    "                    acc1[0],\n",
    "                    epoch * len(val_loader) + i)\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1087af18-507d-40d2-bca3-b2aed22a31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##From PyTorch docs https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html \n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f57e88-c515-49a2-8c21-1931c0bdeeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_init(train_dir, val_dir, num_classes):\n",
    "    model_ft, IMG_SIZE = initialize_model(ARCH, num_classes, False, use_pretrained=False)\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(IMG_SIZE),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_mean_RGB, imagenet_std_RGB)\n",
    "    ])\n",
    "\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_mean_RGB, imagenet_std_RGB)\n",
    "    ])\n",
    "\n",
    "    torch.cuda.set_device(GPU)\n",
    "    model_ft.cuda(GPU)\n",
    "    criterion = nn.CrossEntropyLoss().cuda(GPU)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model_ft.parameters(),\n",
    "        lr=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        momentum=MOMENTUM\n",
    "    )\n",
    "    \n",
    "    model = model_ft\n",
    "\n",
    "    # use CosineAnnealingLR\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=EPOCHS)\n",
    "    \n",
    "    train_dataset = torchvision.datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "    val_dataset = torchvision.datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=BATCH_SIZE, shuffle=True) \n",
    "\n",
    "    return (model, criterion, optimizer, scheduler, scaler, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd99c9f6-b566-4b50-8d20-c7dc05d19523",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_CHECKPOINT_PATH = \"./checkpoint_temp.pth.tar\"\n",
    "DEWP_CHECKPOINT_PATH = \"./checkpoint_dewp.pth.tar\"\n",
    "VISIB_CHECKPOINT_PATH = \"./checkpoint_visib.pth.tar\"\n",
    "WDSP_CHECKPOINT_PATH = \"./checkpoint_wdsp.pth.tar\"\n",
    "\n",
    "TEMP_training_dir = \"data/training/TEMP_class\"\n",
    "TEMP_test_dir = \"data/test/TEMP_class\"\n",
    "DEWP_training_dir = \"data/training/DEWP_class\"\n",
    "DEWP_test_dir = \"data/test/DEWP_class\"\n",
    "VISIB_training_dir = \"data/training/VISIB_class\"\n",
    "VISIB_test_dir = \"data/test/VISIB_class\"\n",
    "WDSP_training_dir = \"data/training/WDSP_class\"\n",
    "WDSP_test_dir = \"data/test/WDSP_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f228091-0fff-476f-a7f1-fdd75e1483ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 0/17]\tTime  1.057 ( 1.057)\tData  0.000 ( 0.000)\tLoss 1.1461e+00 (1.1461e+00)\tAcc@1  26.00 ( 26.00)\tAcc@2  59.00 ( 59.00)\n",
      "Test: [0/4]\tTime  0.786 ( 0.786)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  41.00 ( 41.00)\tAcc@2  79.00 ( 79.00)\n",
      "Learning rate: [0.0004877641290737884]\n",
      "Time elapsed: 0:00:17.728993\n",
      "Epoch: [1][ 0/17]\tTime  0.858 ( 0.858)\tData  0.000 ( 0.000)\tLoss 1.1020e+00 (1.1020e+00)\tAcc@1  37.00 ( 37.00)\tAcc@2  74.00 ( 74.00)\n",
      "Test: [0/4]\tTime  0.756 ( 0.756)\tLoss 1.0457e+00 (1.0457e+00)\tAcc@1  42.00 ( 42.00)\tAcc@2  83.00 ( 83.00)\n",
      "Learning rate: [0.0004522542485937368]\n",
      "Time elapsed: 0:00:35.061094\n",
      "Epoch: [2][ 0/17]\tTime  0.867 ( 0.867)\tData  0.000 ( 0.000)\tLoss 1.0705e+00 (1.0705e+00)\tAcc@1  40.00 ( 40.00)\tAcc@2  79.00 ( 79.00)\n",
      "Test: [0/4]\tTime  0.776 ( 0.776)\tLoss 1.0700e+00 (1.0700e+00)\tAcc@1  39.00 ( 39.00)\tAcc@2  78.00 ( 78.00)\n",
      "Learning rate: [0.0003969463130731183]\n",
      "Time elapsed: 0:00:52.502455\n",
      "Epoch: [3][ 0/17]\tTime  0.872 ( 0.872)\tData  0.000 ( 0.000)\tLoss 1.0442e+00 (1.0442e+00)\tAcc@1  49.00 ( 49.00)\tAcc@2  78.00 ( 78.00)\n",
      "Test: [0/4]\tTime  0.771 ( 0.771)\tLoss 1.0417e+00 (1.0417e+00)\tAcc@1  45.00 ( 45.00)\tAcc@2  80.00 ( 80.00)\n",
      "Learning rate: [0.00032725424859373687]\n",
      "Time elapsed: 0:01:09.845216\n",
      "Epoch: [4][ 0/17]\tTime  0.873 ( 0.873)\tData  0.000 ( 0.000)\tLoss 1.0488e+00 (1.0488e+00)\tAcc@1  45.00 ( 45.00)\tAcc@2  75.00 ( 75.00)\n",
      "Test: [0/4]\tTime  0.766 ( 0.766)\tLoss 1.0696e+00 (1.0696e+00)\tAcc@1  42.00 ( 42.00)\tAcc@2  79.00 ( 79.00)\n",
      "Learning rate: [0.00025]\n",
      "Time elapsed: 0:01:27.254497\n",
      "Epoch: [5][ 0/17]\tTime  0.882 ( 0.882)\tData  0.000 ( 0.000)\tLoss 1.0936e+00 (1.0936e+00)\tAcc@1  37.00 ( 37.00)\tAcc@2  73.00 ( 73.00)\n",
      "Test: [0/4]\tTime  0.793 ( 0.793)\tLoss 1.0181e+00 (1.0181e+00)\tAcc@1  47.00 ( 47.00)\tAcc@2  81.00 ( 81.00)\n",
      "Learning rate: [0.00017274575140626317]\n",
      "Time elapsed: 0:01:44.797273\n",
      "Epoch: [6][ 0/17]\tTime  0.858 ( 0.858)\tData  0.000 ( 0.000)\tLoss 1.0574e+00 (1.0574e+00)\tAcc@1  46.00 ( 46.00)\tAcc@2  76.00 ( 76.00)\n",
      "Test: [0/4]\tTime  0.765 ( 0.765)\tLoss 1.0507e+00 (1.0507e+00)\tAcc@1  44.00 ( 44.00)\tAcc@2  80.00 ( 80.00)\n",
      "Learning rate: [0.00010305368692688175]\n",
      "Time elapsed: 0:02:02.236140\n",
      "Epoch: [7][ 0/17]\tTime  0.874 ( 0.874)\tData  0.000 ( 0.000)\tLoss 1.0761e+00 (1.0761e+00)\tAcc@1  38.00 ( 38.00)\tAcc@2  76.00 ( 76.00)\n",
      "Test: [0/4]\tTime  0.770 ( 0.770)\tLoss 1.0493e+00 (1.0493e+00)\tAcc@1  49.00 ( 49.00)\tAcc@2  77.00 ( 77.00)\n",
      "Learning rate: [4.774575140626317e-05]\n",
      "Time elapsed: 0:02:19.688730\n",
      "Epoch: [8][ 0/17]\tTime  0.863 ( 0.863)\tData  0.000 ( 0.000)\tLoss 1.0111e+00 (1.0111e+00)\tAcc@1  49.00 ( 49.00)\tAcc@2  86.00 ( 86.00)\n",
      "Test: [0/4]\tTime  0.766 ( 0.766)\tLoss 1.0679e+00 (1.0679e+00)\tAcc@1  44.00 ( 44.00)\tAcc@2  80.00 ( 80.00)\n",
      "Learning rate: [1.2235870926211617e-05]\n",
      "Time elapsed: 0:02:37.130482\n",
      "Epoch: [9][ 0/17]\tTime  0.863 ( 0.863)\tData  0.000 ( 0.000)\tLoss 1.0930e+00 (1.0930e+00)\tAcc@1  44.00 ( 44.00)\tAcc@2  69.00 ( 69.00)\n",
      "Test: [0/4]\tTime  0.763 ( 0.763)\tLoss 1.0654e+00 (1.0654e+00)\tAcc@1  39.00 ( 39.00)\tAcc@2  75.00 ( 75.00)\n",
      "Learning rate: [0.0]\n",
      "Time elapsed: 0:02:54.569723\n"
     ]
    }
   ],
   "source": [
    "model, criterion, optimizer, scheduler, scaler, train_loader, val_loader = single_init(DEWP_training_dir, DEWP_test_dir, num_classes=3)\n",
    "    \n",
    "start = datetime.now()\n",
    "\n",
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "\n",
    "    train(train_loader, model, criterion, optimizer, scaler, epoch)\n",
    "    validate(val_loader, model, criterion, epoch)\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': ARCH,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, filename=DEWP_CHECKPOINT_PATH)\n",
    "\n",
    "    scheduler.step()\n",
    "    print('Learning rate: ' + str(scheduler.get_last_lr()))\n",
    "    print(\"Time elapsed: \" + str(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6526f13-32a4-4b42-a2a1-82bde984e1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130b9e3-8220-47a2-990a-6d39a7d05d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ef59f-f632-4ac2-96e2-c7ceb687c4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
